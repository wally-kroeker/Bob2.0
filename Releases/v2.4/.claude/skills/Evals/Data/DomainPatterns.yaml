# Domain-Specific Grader Patterns
# Based on Anthropic's "Demystifying Evals for AI Agents"

domains:

  coding:
    description: "Coding agent evaluation"
    primary_graders:
      - type: binary_tests
        weight: 0.30
        description: "Unit tests must pass"
      - type: static_analysis
        weight: 0.15
        params:
          commands: [biome, tsc]
      - type: tool_calls
        weight: 0.20
        params:
          sequence: [read_file, edit_file, run_tests]
      - type: llm_rubric
        weight: 0.25
        params:
          rubric: |
            Evaluate the code change for:
            - Correctness: Does it solve the problem?
            - Style: Does it follow project conventions?
            - Safety: Does it avoid introducing bugs?
      - type: state_check
        weight: 0.10
    tracked_metrics:
      - type: transcript
        metrics: [n_turns, n_toolcalls, n_total_tokens]

  conversational:
    description: "Conversational agent evaluation"
    primary_graders:
      - type: llm_rubric
        weight: 0.40
        params:
          rubric: |
            Evaluate the conversation for:
            - Task Resolution: Was the user's issue resolved?
            - Empathy: Did the agent acknowledge frustration?
            - Clarity: Were explanations clear?
            - Groundedness: Were claims backed by tool results?
      - type: natural_language_assert
        weight: 0.25
        params:
          assertions:
            - "Agent showed empathy for customer's situation"
            - "Resolution was clearly explained"
            - "Agent's response was grounded in tool results"
      - type: state_check
        weight: 0.15
        params:
          expect:
            status: resolved
      - type: tool_calls
        weight: 0.20
        params:
          required:
            - tool: verify_identity
            - tool: send_confirmation
    tracked_metrics:
      - type: transcript
        metrics: [n_turns]
    constraints:
      max_turns: 10

  research:
    description: "Research agent evaluation"
    primary_graders:
      - type: llm_rubric
        weight: 0.50
        params:
          rubric: |
            Evaluate the research output for:
            - Groundedness: Are claims backed by sources?
            - Coverage: Are key topics addressed?
            - Source Quality: Are sources reputable?
            - Synthesis: Is information well-organized?
      - type: natural_language_assert
        weight: 0.30
        params:
          assertions:
            - "Claims are backed by cited sources"
            - "Multiple perspectives are considered"
            - "Sources are from reputable domains"
      - type: tool_calls
        weight: 0.20
        params:
          required:
            - tool: web_search
            - tool: read_source
    tracked_metrics:
      - type: transcript
        metrics: [n_turns, n_toolcalls]

  computer_use:
    description: "Computer/GUI agent evaluation"
    primary_graders:
      - type: state_check
        weight: 0.40
        params:
          verify_gui: true
      - type: tool_calls
        weight: 0.30
        params:
          required:
            - tool: screenshot
            - tool: click
      - type: llm_rubric
        weight: 0.30
        params:
          rubric: |
            Evaluate the GUI interaction for:
            - Efficiency: Were actions minimal and direct?
            - Correctness: Was the target element correct?
            - Error Handling: Were errors handled gracefully?
    tracked_metrics:
      - type: latency
        metrics: [time_to_last_token]

  general:
    description: "General-purpose evaluation"
    primary_graders:
      - type: llm_rubric
        weight: 0.50
        params:
          rubric: |
            Evaluate the output for:
            - Accuracy: Is the information correct?
            - Completeness: Is the task fully addressed?
            - Quality: Is the output well-structured?
      - type: natural_language_assert
        weight: 0.30
        params:
          assertions:
            - "The output addresses the user's request"
            - "The information appears accurate"
      - type: tool_calls
        weight: 0.20
    tracked_metrics:
      - type: transcript
        metrics: [n_turns, n_toolcalls]

# Default thresholds by eval type
thresholds:
  capability:
    pass_threshold: 0.70
    saturation_threshold: 0.95
    description: "Stretch goals - expecting <100% pass rate"

  regression:
    pass_threshold: 0.95
    alert_threshold: 0.90
    description: "Quality gates - must maintain near 100%"
